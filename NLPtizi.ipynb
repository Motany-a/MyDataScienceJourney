{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                           TRYING NLP OUT üòè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importing nltk(an NLP Library)\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing TextBlob(an NLP  library)\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"Analytics Vidhya is a great platform to learn data science. \\n It helps community through blogs, hackathons, discussions,etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"Analytics Vidhya is a great platform to learn data science.\"), Sentence(\"It helps community through blogs, hackathons, discussions,etc.\")]\n"
     ]
    }
   ],
   "source": [
    "print(blob.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics\n",
      "Vidhya\n",
      "is\n",
      "a\n",
      "great\n",
      "platform\n",
      "to\n",
      "learn\n",
      "data\n",
      "science\n"
     ]
    }
   ],
   "source": [
    "## printing words of first sentence\n",
    "for words in blob.sentences[0].words:  \n",
    "  print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytics vidhya\n",
      "great platform\n",
      "data science\n"
     ]
    }
   ],
   "source": [
    "##Extracting noun phases\n",
    "for np in blob.noun_phrases:\n",
    " print (np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics NNS\n",
      "Vidhya NNP\n",
      "is VBZ\n",
      "a DT\n",
      "great JJ\n",
      "platform NN\n",
      "to TO\n",
      "learn VB\n",
      "data NNS\n",
      "science NN\n",
      "It PRP\n",
      "helps VBZ\n",
      "community NN\n",
      "through IN\n",
      "blogs NNS\n",
      "hackathons NNS\n",
      "discussions NNS\n",
      "etc FW\n"
     ]
    }
   ],
   "source": [
    "##This is just a complete version of noun phrase extraction, where we want to find all the the parts of speech in a sentence.\n",
    "for words, tag in blob.tags:\n",
    " print (words, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helps\n",
      "help\n"
     ]
    }
   ],
   "source": [
    "## Word inflection\n",
    "## THis means adding characters to base form of word to express gramatical meanings\n",
    "print (blob.sentences[1].words[1])\n",
    "print (blob.sentences[1].words[1].singularize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## inflection using TextBlob object called 'Word'\n",
    "from textblob import Word\n",
    "w = Word('Platform')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platforms\n",
      "sciences\n",
      "communities\n"
     ]
    }
   ],
   "source": [
    "## using tags to inflect\n",
    "for word,pos in blob.tags:\n",
    " if pos == 'NN':\n",
    "     print (word.pluralize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lemmatization using a function\n",
    "## Lemmatization is reducing words to their stem words then checking if they are dictionary words\n",
    "w = Word('running')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Analytics', 'Vidhya']\n",
      "['Vidhya', 'is']\n",
      "['is', 'a']\n",
      "['a', 'great']\n",
      "['great', 'platform']\n",
      "['platform', 'to']\n",
      "['to', 'learn']\n",
      "['learn', 'data']\n",
      "['data', 'science']\n",
      "['science', 'It']\n",
      "['It', 'helps']\n",
      "['helps', 'community']\n",
      "['community', 'through']\n",
      "['through', 'blogs']\n",
      "['blogs', 'hackathons']\n",
      "['hackathons', 'discussions']\n",
      "['discussions', 'etc']\n"
     ]
    }
   ],
   "source": [
    "## Using N grams\n",
    "## N-grams are a combination of multiple words used together.\n",
    "for ngram in blob.ngrams(2):\n",
    "    print (ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics Vidhya is a great platform to learn data science. \n",
      " It helps community through blogs, hackathons, discussions,etc.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=0.75)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sentiment analysis\n",
    "## Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.\n",
    "print (blob)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 Style=\"color:green\">We can see that polarity is 0.8, which means that the statement is positive and 0.75 subjectivity refers that mostly it is a public opinion and not a factual information.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Analytics Vidhya is a great platform to learn data science\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## spelling correction\n",
    "blob = TextBlob('Analytics Vidhya is a gret platfrm to learn data scence')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.5351351351351351),\n",
       " ('get', 0.3162162162162162),\n",
       " ('grew', 0.11216216216216217),\n",
       " ('grey', 0.026351351351351353),\n",
       " ('greet', 0.006081081081081081),\n",
       " ('fret', 0.002702702702702703),\n",
       " ('grit', 0.0006756756756756757),\n",
       " ('cret', 0.0006756756756756757)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the suggested words and their confidence\n",
    "blob.words[4].spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating a short summary of a paragraph</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "blob = TextBlob('Analytics Vidhya is a thriving community for data driven industry. This platform allows \\\n",
    "people to know more about analytics from its articles, Q&A forum, and learning paths. Also, we help \\\n",
    "professionals & amateurs to sharpen their skillsets by providing a platform to participate in Hackathons.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is about...\n",
      "platforms\n",
      "industries\n",
      "communities\n",
      "platforms\n",
      "forums\n"
     ]
    }
   ],
   "source": [
    "nouns = list()\n",
    "for word, tag in blob.tags:\n",
    "    if tag == 'NN':\n",
    "        nouns.append(word.lemmatize())\n",
    "\n",
    "print (\"This text is about...\")\n",
    "for item in random.sample(nouns, 5):\n",
    "    word = Word(item)\n",
    "    print (word.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text classification using TextBlob</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [\n",
    "('Tom Holland is a terrible spiderman.','pos'),\n",
    "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "('Fantastic Four should have never been made.','pos'),\n",
    "('Wes Anderson is my favorite director!','neg'),\n",
    "('Captain America 2 is pretty awesome.','neg'),\n",
    "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "testing = [\n",
    "('Superman was never an interesting character.','pos'),\n",
    "('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "('Dragonball Evolution is simply terrible!!','pos')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textblob import classifiers\n",
    "classifier = classifiers.NaiveBayesClassifier(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Most Informative Features\n",
      "            contains(is) = True              neg : pos    =      2.9 : 1.0\n",
      "             contains(a) = False             neg : pos    =      1.8 : 1.0\n",
      "         contains(never) = False             neg : pos    =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print (classifier.accuracy(testing))\n",
    "classifier.show_informative_features(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h5 Style=\"color:white\">code excerpts from www.analyticsvidhya.com</h5>"
     ]
   }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
